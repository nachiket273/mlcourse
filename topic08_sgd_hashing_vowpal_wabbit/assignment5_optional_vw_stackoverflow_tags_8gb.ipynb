{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Authors: [Yury Kashnitsky](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Assignment 5. Optional part\n",
    "## <center> Vowpal Wabbit for Stackoverflow question tag classification\n",
    "    \n",
    "        \n",
    "#  <center>  <font color = 'red'> Warning! </font>This is a very useful but ungraded assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "   1. [Introduction](#1.-Introduction)\n",
    "   2. [Data description](#2.-Data-description)\n",
    "   3. [Data preprocessing](#3.-Data-preprocessing)\n",
    "   4. [Training and validation](#4.-Training-and-validation)\n",
    "   5. [Notes](#5.-Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "In this task you'll be doing the same thing that I did at Mail.ru Group – training models with gigabytes of data. You can try to stick to Python and Windows environment, but we strongly recommend some \\*NIX-system (with, for instance, Docker) and use bash actively there. Having some experience with bash and UNIX utils is a very important skill for a data scientist.\n",
    "\n",
    "For this particular task we need Vowpal Wabbit installed (we provide it with docker-container, instructions are given [here](https://mlcourse.ai/prerequisites)) and approximately  50 GB of disk space. I tested solution on ordinary Macbook Pro 2015 (8 cores, 16 GB RAM), the heaviest model was trained in under 12 min, so the task is doable with quite usual hardware.\n",
    "\n",
    "\n",
    "Supplementary stuff:\n",
    " - interactive [tutorial](https://www.codecademy.com/en/courses/learn-the-command-line/lessons/environment/exercises/bash-profile) from CodeAcademy on UNIX command line (1-2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 GB of questions from StackOverflow split into 75% train and 25% test parts. You can download the training part [from here](https://drive.google.com/file/d/1w8z6HmFe4oCQSG6DjomSRUWvJ-gK0LTe/view?usp=sharing) (~2.5 GB archived, ~8 GB unpacked).\n",
    "\n",
    "Data format is simple:<br>\n",
    "<center>*question text* (space delimited words) TAB *question tags* (space delimited)\n",
    "\n",
    "TAB – is a tabulation symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize this\n",
    "PATH_TO_DATA = '../../data/stackoverflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First sample from training set for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 $PATH_TO_DATA/stackoverflow_raw_train_7500k.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have question text, then tab and question tags: *css, css3* and *css-selectors*. And so we have 7.5 mln such questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!wc -l $PATH_TO_DATA/stackoverflow_raw_train_7500k.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we are not going to load this dataset into memory at any point, feel free to use Unix utilities - `head`, `tail`, `wc`, `cat`, `cut`, etc. to explore and process the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select all questions with tags *javascript, java, python, ruby, php, c++, c#, go, scala* and  *swift* from the data source and prepare a training set in the Vowpal Wabbit data format. We be solving a 10-class classification problem: each question can be tagged with one of these tags. \n",
    "\n",
    "Generally, as we see, questions may have several tags, but we will simplify our task selecting only questions having one of the tags from the list.\n",
    "\n",
    "However, it's good to know that VW supports multilabel classification (`--multilabel_oaa` parameter).\n",
    "<br>\n",
    "<br>\n",
    "Implement data preprocessing code in separate file `preprocess.py`. This script is going to select all lines with tags *javascript, java, python, ruby, php, c++, c#, go, scala*, *swift* and write them to a file in VW format. Details:\n",
    " - the script takes command line arguments: input and output file paths \n",
    " - lines are processed one-by-one (you can use `tqdm` to track iterations)\n",
    " - if a line has no tabs or more than one tab  - then the line is broken, skip it\n",
    " - if line has exactly one tab symbol, check, how many tags from list *javascript, java, python, ruby, php, c++, c#, go, scala* or  *swift* are there. If there is only one tag - write string to output with VW format: `label | text`, where `label` – number from 1 to 10 (1 - *javascript*, ... 10 – *swift*). Skip strings with no tags or more than one tag from our list.\n",
    " - remove `:` and `|` symbols from question text - these are reserved VW symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You should get 3291403 lines in the processed data file. In our case Python processes 8 GB in ~1.5 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python preprocessor.py -ip_fp $PATH_TO_DATA/stackoverflow_raw_train_7500k.tsv \\\n",
    "    -op_fp $PATH_TO_DATA/stackoverflow_raw_train_7500k.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!wc -l $PATH_TO_DATA/stackoverflow_raw_train_7500k.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 $PATH_TO_DATA/stackoverflow_raw_train_7500k.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into training, and validation parts  -  approx. 2/3 shall go to the training - 2194270 lines. We don't need to shuffle the data, first 2194270 lines go into training part `stackoverflow_train.vw`, last 1097133 lines – to the validation test part `stackoverflow_valid.vw`. \n",
    "\n",
    "Also, save a vector with correct labels for the validation set into a separate files `stackoverflow_valid_labels.txt`.\n",
    "\n",
    "Use `head`, `tail`, `split`, `cat` and `cut` linux utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!split $PATH_TO_DATA/stackoverflow_raw_train_7500k.vw  $PATH_TO_DATA/stackoverflow_processed -l 2038856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv $PATH_TO_DATA/stackoverflow_processedaa $PATH_TO_DATA/stackoverflow_train.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $PATH_TO_DATA/stackoverflow_processedab | cut -d ' ' -f 2- > $PATH_TO_DATA/stackoverflow_valid.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $PATH_TO_DATA/stackoverflow_processedab | cut -d ' ' -f 1 > $PATH_TO_DATA/stackoverflow_valid_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Vowpal Wabbit with `stackoverflow_train.vw` 9 times, changing the number of `passes` (1,3,5) and `ngram` (1,2,3). The rest parameters are: `bit_precision`=28 and `seed`=17. Also tell VW, that it is a 10-class classification problem that we have.\n",
    "\n",
    "Evaluate accuracy with `stackoverflow_valid.vw` and select best hyperparams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Question.</font> Which parameter set provides the best accuracy on validation set `stackoverflow_valid.vw`?\n",
    "- bigrams (`ngram`=2) and 3 epochs (`passes`=3)\n",
    "- trigrams and 5 epochs\n",
    "- bigrams and 1 epoch\n",
    "- unigrams and 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bigrams and 1 epoch**\n",
    "**93.3%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$PATH_TO_DATA/train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1, N-grams: 1\n",
      "Accuracy: 0.919\n",
      "Epochs: 1, N-grams: 2\n",
      "Accuracy: 0.933\n",
      "Epochs: 1, N-grams: 3\n",
      "Accuracy: 0.932\n",
      "Epochs: 3, N-grams: 1\n",
      "Accuracy: 0.919\n",
      "Epochs: 3, N-grams: 2\n",
      "Accuracy: 0.931\n",
      "Epochs: 3, N-grams: 3\n",
      "Accuracy: 0.929\n",
      "Epochs: 5, N-grams: 1\n",
      "Accuracy: 0.919\n",
      "Epochs: 5, N-grams: 2\n",
      "Accuracy: 0.932\n",
      "Epochs: 5, N-grams: 3\n",
      "Accuracy: 0.929\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'stackoverflow_valid_labels.txt')) as pred_file:\n",
    "    y_valid = [float(label) for label in pred_file.readlines()]\n",
    "\n",
    "for n_pass in [1, 3, 5]:\n",
    "    for n_gram in [1, 2, 3]:\n",
    "        file_name = 'stackoverflow_valid_pred_' + str(n_pass) + str(n_gram) + '.txt'\n",
    "        with open(os.path.join(PATH_TO_DATA, file_name)) as pred_file:\n",
    "            test_prediction = [float(label) for label in pred_file.readlines()]\n",
    "        print(\"Epochs: {}, N-grams: {}\".format(n_pass, n_gram))\n",
    "        print(\"Accuracy: {}\".format(round(accuracy_score(y_valid, test_prediction), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Notes\n",
    "\n",
    "A note on this task:\n",
    "- in future, there'll be a Kaggle competition organized with this data\n",
    "- we could've used  `sklearn` wrapper for Vowpal Wabbit as shown in [this Kernel](https://www.kaggle.com/kashnitsky/training-while-reading-vowpal-wabbit-starter)\n",
    "- we did not use `hyperopt` package for parameter tuning\n",
    "- it is better to write results in a log file instead of printing\n",
    "- for data preprocessing tasks Linux shell utilities are faster than `Python` scripts\n",
    "\n",
    "However, the solution that you'll get is quite good. And, keeping the data set size in mind, there is no point in heavy hyperparameter tuning. In general, with Vowpal Wabbit you can get reasonable baselines very fast, even in tasks where dataset sizes look intimidating at a first glance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
